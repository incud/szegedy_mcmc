{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5973c7c8-af75-4270-a39a-8c3a8a4a3687",
   "metadata": {},
   "source": [
    "# Notes on the Szegedy Quantum Walks | Introduction to random walks\n",
    "\n",
    "**Table of contents**\n",
    "\n",
    "0. _Introduction to random walks_\n",
    "    1. Markov chains and properties\n",
    "    2. Boltzmann distribution and the Ising model\n",
    "    4. Metropolis-Hastings algorithm\n",
    "    5. Python implementation\n",
    "1. _Introduction to quantum walks_\n",
    "    1. From Markov chains to unitary quantum walks\n",
    "    2. Construction of $W$\n",
    "    3. Python implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371db6e3-219d-4939-b924-e4c176e70be3",
   "metadata": {},
   "source": [
    "## Markov chains and properties\n",
    "\n",
    "A (finite) Markov chain is specified by a state space $\\Omega$ and a transition matrix $P \\in \\mathbb{R}^{|\\Omega| \\times |\\Omega|}$ where \n",
    "$$P(x, y) = \\mathrm{Pr}[X_{t+1} = y \\mid X_t = x]$$ \n",
    "is the probability of moving from state $x$ to $y$. Some properties:\n",
    "\n",
    "* _**column stochasticity**_: each row represents a probability distribution, thus $P_{ij} \\ge 0, \\sum_i P_{ij} = 1$ for $i, j \\in \\Omega$. This fixes also the notation used, with $P(x,y) = P_{yx}$ and a distribution $\\mu$ being a column-vector, with time evolution $\\mu_{t+1} = P \\mu_t$.\n",
    "\n",
    "    (Of course one can use the dual notation in which case the property that needs to hold is the row stochasticity).\n",
    "\n",
    "* **theorem**: a stationary distribution $\\pi$ exists, $\\pi = P \\pi$, for any stochastic matrix $P$ (finite case only).\n",
    "\n",
    "* _**irreducibility**_: the directed graph induced by $P$ is strongly connected, or equivalently $\\forall x, y \\in \\Omega \\; \\exists t \\ge 1 \\,:\\, P^t(x,y) > 0$. This ensures the chain can reach any state.\n",
    "\n",
    "* **theorem**: the chain is irreducible iff the stationary distribution is unique.\n",
    "\n",
    "* _**aperiodicity**_: the _period_ of the state $x$ is $d(x) = \\mathrm{gcd}\\{ t \\ge 1 \\mid P^t_{xx} > 0 \\}$; a markov chain is _aperiodic_ if for all states $x$ we have $d(x) = 1$.\n",
    "\n",
    "* **theorem**: in a _irreducible_ chain, all states have the same period; thus, if _one_ state has a self-loop $P_{xx} > 0$ then the entire chain is aperiodic. If an irreducible chain is aperiodic too, then it converges to the stationary distribution from any initial distribution.\n",
    "\n",
    "* _**detailed balance/reversibility**_: the markov chain satisfies the detailed balance with respect to $\\pi$ iff $\\pi_x P_{yx} = \\pi_y P_{xy} \\; \\forall x,y \\in \\Omega$. In case, the MC is said to be _reversible_\n",
    "\n",
    "* **theorem**: if the irreducible $P$ is reversible then there exists a unique stationary distribution.\n",
    "\n",
    "* **theorem**: if the irreducible $P$ is symmetric then it is reversible by design; not all reversible $P$ are symmetric though.\n",
    "\n",
    "The performance of a MC is quantified via the mixing time, i.e. how quickly the chain approaches stationarity in the worst initial state. One definition uses the total variation norm, \n",
    "$$\\lVert \\mu \\rVert_\\text{TV} =\\sup_x |\\mu_x| $$\n",
    "Let $\\mu_t^{(x)} $ be the distribution at time $t$ starting from the initial state $x$ and $\\pi$ the unique stationary distribution. The **$\\varepsilon$-mixing time** is \n",
    "$$t_\\text{mix}(\\varepsilon) = \\min \\{t \\mid \\max_{x \\in \\Omega} \\lVert \\mu_t^{(x)} - \\pi \\rVert_\\text{TV} \\le \\varepsilon \\}$$\n",
    "For finite irreducible reversible chains, convergence rates are closely controlled by the spectral gap. Let the eigenvalues of $P$ be $1 = \\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_N \\ge -1$. The spectral gap is \n",
    "$$\\gamma := 1 - \\max \\{|\\lambda_2|, |\\lambda_{N}| \\}$$\n",
    "\n",
    "(when the chain is lazy or otherwise has nonnegative spectrum, the simpler form $\\gamma := 1 - \\lambda_2$ may be used)\n",
    "\n",
    "Then, $t_\\text{mix}(\\epsilon) \\le \\gamma^{-1}$ up to logarithmic terms in $\\varepsilon^{-1}$ and $\\pi^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee6215-3656-4432-a61e-102ec1316529",
   "metadata": {},
   "source": [
    "## Boltzmann distribution and the Ising model\n",
    "\n",
    "Let $\\Omega$ be a finite configuration space and let the Hamiltonian be a real-valued function $E: \\Omega \\to \\mathbb{R}$ assigning an energy value to each configuration. Let $\\beta \\ge 0$ be an inverse termperature parameter. The Boltzmann distribution $\\mu_\\beta$ is \n",
    "$$\\mu_\\beta(x) := \\frac{e^{-\\beta E(x)}}{Z(\\beta)}$$\n",
    "where $Z(\\beta) := \\sum_x e^{-\\beta E(x)}$ is the partition function. \n",
    "\n",
    "The parameter $\\beta$ controls the concentration: at high temperatures ($\\beta \\to 0$) we have $\\mu_\\beta$ approaching uniform distribution and at low temperatures ($\\beta \\to \\infty$) it concetrates around the ground states of $E$.\n",
    "\n",
    "A notable example of models we can study are the Ising model. We classify them here by geometry:\n",
    "\n",
    "The Ising model on a graph $G = (V, E)$ has variables $x_i \\in \\{ \\pm 1 \\}$ for $i \\in V$ and the energy term can be expressed as\n",
    "$$E(x) = - \\sum_{\\{i,j\\} \\in E} J_{ij} x_i x_j - h_i x_i$$\n",
    "This means there are 2-body terms only (here the notation is $k=2$ where $k$ refers to the number of interacting particles). A sub-class of these is the lattice Ising which has a further restriction on the graph being a grid with regular structure, this imposes a bounded interaction neighborhood (here the notation is $d$).\n",
    "\n",
    "The $(k, d)$-local Ising model has variables $x_i \\in \\{ \\pm 1 \\}$ for $i = 1, ..., n$ and the energy term can be expressed as\n",
    "$$E(x) = - \\sum_\\ell J_\\ell \\prod_{s \\in \\Omega_\\ell} x_s$$\n",
    "where $\\Omega_\\ell \\subseteq [n]$ contains at most $k$ spin, and each spin interacts with at most $d$ other spins, i.e. the dependency neighborhood is local. \n",
    "\n",
    "It is not true that the Ising on graph is necessarily a sub-class of $(k,d)$-local Ising for $k, d$ constants, as the graph in the former can have large degree ($d \\approx n$) on some vertices thus the dependency neighborhood is large. A lattice ising is equivalent to $(k=2, d=4)$-local Ising. \n",
    "\n",
    "**In these notes it is paramount to consider $(k,d)$-local Ising as that's the format we can optimize the most.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88d272-72b8-450a-88ca-8075ce392b3c",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings algorithm\n",
    "\n",
    "This is a general framework to construct a MC whose stationary distribution is the desider target distribution $\\pi$, might be a Boltzmann distribution $\\pi_\\beta$ or it might be something else. \n",
    "\n",
    "Let $\\Omega$ be the state space and $\\pi$ be a distribution on $\\Omega$ with $\\pi_x > 0$ for all $x$ (we want no singularities caused by $\\pi_x^{-1}$). \n",
    "\n",
    "We start by defining a column-stochastic **proposal matrix** $A$ that is the probability of proposing a move from $x$ to $y$. For example, for an Ising model it could be\n",
    "$$A(x, y) = A_{yx} = \\begin{cases} \\frac{1}{n+1}, & y = x \\oplus 2^j, \\quad j \\in [n] \\\\ 0, & y = x \\\\ 0, & \\text{otherwise} \\end{cases}$$\n",
    "which basically is the uniform single spin-flip model: $x \\oplus 2^j$ means flip the $j$-th spin, and the probability of doing so is 1/number of spins. \n",
    "\n",
    "The single spin flip model is a great example: it is local (a constant amount of spin to flip at each move, not growing with $n$, lead to efficient optimizations later), and symmetric (implies reversibility). \n",
    "\n",
    "Given $y \\sim A(x, \\cdot)$ the move is accepted with probability\n",
    "$$\\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y) A(y, x)}{\\pi(x) A(x, y)} \\right)$$\n",
    "Therefore\n",
    "$$\\begin{align*}\n",
    "P(x, y) & = A(x, y) \\alpha(x, y), & x \\neq y, \\\\\n",
    "P(x, x) & = 1 - \\sum_{y \\neq x} A(x, y) \\alpha(x, y),\n",
    "\\end{align*}$$\n",
    "By construction $P$ is column stochastic, enforces detailed balance, and includes self loops (aperiodic). If $A$ is irreducible, which basically is the only thing you need to prove by yourself, the constructed $P$ is irreducible, reversible, aperiodic: _it has a unique stationary state reachable by any initial state_. \n",
    "\n",
    "In the _special case_ of the Boltzmann distribution, the acceptance probability depends _uniquely_ on the energy difference:\n",
    "$$\\alpha(x, y) = \\min\\left( 1, e^{-\\beta(E(y) - E(x))} \\cdot \\frac{A(y, x)}{A(x, y)} \\right)$$\n",
    "In this case, one commonly chooses $A$ to be symmetric so to simply this to the _Metropolis rule_:\n",
    "$$\\alpha(x, y) = \\min\\left( 1, e^{-\\beta(E(y) - E(x))} \\right)$$\n",
    "i.e. always accept energy lowering moves, and accept energy increasing moves with probability exponentially suppressed in $\\beta$. \n",
    "\n",
    "**Important**: this method has avoided us to calculate the partition function, which is great because it is unfeasible in the worst case and an annoyance in the best. \n",
    "\n",
    "**Important**: the efficiency is governed by the mixing time: complex energy landscapes as low-temperature Ising, the acceptance probabilities becomes small and the spectral gap drops. The convergence might be very slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60091846-7ab1-4707-aaa2-889f1d0b6d61",
   "metadata": {},
   "source": [
    "## Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844e829-7388-4aaa-a5b4-bad280bf6f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
